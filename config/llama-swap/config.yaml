models:
  "Qwen3-4B-Thinking-2507-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/qwen3-4b-thinking.gguf
      --port ${PORT}
      --ctx-size 32768
      -ctk q8_0
      -ctv q8_0
      --n-gpu-layers -1
      --numa isolate
      --flash-attn
      --jinja
  "Qwen3-4B-Instruct-2507-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/qwen3-4b-instruct-q8_0.gguf
      --port ${PORT}
      --ctx-size 32768
      -ctk q8_0
      -ctv q8_0
      --n-gpu-layers -1
      --numa isolate
      --flash-attn
      --jinja
  "Qwen3-30B-Thinking-2507-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/qwen3-30b-thinking-q8_0.gguf
      --port ${PORT}
      --ctx-size 32768
      -ctk q8_0
      -ctv q8_0
      --n-gpu-layers -1
      --numa isolate
      --flash-attn
      --jinja
  "Qwen3-30B-Instruct-2507-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/qwen3-30b-instruct-q8_0.gguf
      --port ${PORT}
      --ctx-size 32768
      -ctk q8_0
      -ctv q8_0
      --n-gpu-layers -1
      --numa isolate
      --flash-attn
      --jinja
  "gpt-oss-20b-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/gpt-oss-20b.gguf
      --port ${PORT}
      --ctx-size 32768
      -ctk q8_0
      -ctv q8_0
      --n-gpu-layers -1
      --numa isolate
      --flash-attn
      --jinja
  "gemma-3-12b-it-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/gemma3-12b.gguf
      --mmproj models/gemma3-12b-mmproj.gguf
      --port ${PORT}
      --ctx-size 32768
      -ctk q8_0
      -ctv q8_0
      --n-gpu-layers -1
      --numa isolate
      --flash-attn
      --jinja
  "Qwen3-0.6B-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/Qwen3-0.6B-Q8_0.gguf
      --port ${PORT}
      --n-gpu-layers -1
  "Qwen3-Embedding-0.6B-Q8_0":
    cmd: |
      llama.cpp/build/bin/llama-server
      --model models/Qwen3-Embedding-0.6B-Q8_0.gguf
      --port ${PORT}
      --embeddings
      --n-gpu-layers -1
groups:
  "forever":
    persistent: true
    members:
      - "Qwen3-Embedding-0.6B-Q8_0"
      - "Qwen3-0.6B-Q8_0"
